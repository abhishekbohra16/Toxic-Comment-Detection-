# -*- coding: utf-8 -*-
"""nlp work  Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9yy_wn3oPKzJxzO9EEL6TyPdSO6z4mX
"""

!pip install gensim

# Step 1: Mount Google Drive and Load Dataset

from google.colab import drive
import pandas as pd

df = pd.read_csv("path_to_dataset.csv")

# Mount Drive
drive.mount('/content/drive', force_remount=True)

# âœ… Path to your dataset
DATA_PATH = "/content/drive/MyDrive/train.csv"  # change if your file is in another folder

# Load dataset - Loading the full dataset now
df = pd.read_csv(DATA_PATH)


# Display dataset info
print("âœ… Dataset Loaded Successfully!\n")
print("Shape:", df.shape)
print("Columns:", df.columns.tolist(), "\n")

# Show first few rows
display(df.head())

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download NLTK dependencies
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Text cleaning function
def clean_text(text):
    text = text.lower()                            # Lowercase
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\@\w+|\#', '', text)           # Remove mentions/hashtags
    text = re.sub(r'[^a-z\s]', '', text)           # Remove punctuation/numbers
    text = re.sub(r'\s+', ' ', text).strip()       # Remove extra spaces
    return text

# Remove stopwords and lemmatize
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words)


# Load dataset
# df = pd.read_csv(DATA_PATH)  # update path if needed


# Apply cleaning
df['clean_text'] = df['comment_text'].apply(clean_text)


df['clean_text'] = df['clean_text'].apply(preprocess_text)

# Check final dataset
display(df[['comment_text', 'clean_text']].head())

# Save preprocessed data for model use
df.to_csv("preprocessed_train.csv", index=False)
print("âœ… Preprocessing complete and saved as 'preprocessed_train.csv'")

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

# Load the preprocessed dataset
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text'
df.dropna(subset=['clean_text'], inplace=True)

# Use only the cleaned text column
X = df['clean_text']

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2))  # unigrams + bigrams

# Fit and transform on the full text data
X_tfidf = tfidf.fit_transform(X)

# Check shape
print("âœ… TF-IDF shape:", X_tfidf.shape)

# Show sample feature names
print("\nSample feature names:")
print(tfidf.get_feature_names_out()[:20])

# Save for later Logistic Regression model
joblib.dump(tfidf, "tfidf_vectorizer.pkl")
joblib.dump(X_tfidf, "X_tfidf.pkl")

print("\nâœ… TF-IDF features generated and saved successfully!")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd # Import pandas to load the dataframe

# Load the preprocessed dataset to get the labels
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text' to match X_tfidf
df.dropna(subset=['clean_text'], inplace=True)

# Define labels and y
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Ensure same number of samples
# y = y.iloc[:len(X_tfidf)] # Remove this line as it's no longer needed after dropping NaNs from df
# X_tfidf is already loaded from the previous cell's execution

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y, test_size=0.2, random_state=42
)

# Train Logistic Regression for each label (balanced)
balanced_lr_models = {}

for label in labels:  # labels = list of your output columns
    print(f"\nðŸš€ Training Balanced Logistic Regression for '{label}'...")

    lr = LogisticRegression(max_iter=200, n_jobs=-1, class_weight='balanced')
    lr.fit(X_train, y_train[label])
    y_pred = lr.predict(X_test)

    print(f"âœ… Accuracy for {label}: {accuracy_score(y_test[label], y_pred):.4f}")
    print(classification_report(y_test[label], y_pred))

    balanced_lr_models[label] = lr

print("\nâœ… All Logistic Regression models trained successfully with class balancing!")

from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Assuming balanced_lr_models dictionary and y_test are available from the previous step

evaluation_results = {}

print("ðŸš€ Evaluating Balanced Logistic Regression Models...")

for label, model in balanced_lr_models.items():
    print(f"\nEvaluating model for '{label}'...")
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test[label], y_pred)
    report = classification_report(y_test[label], y_pred, output_dict=True)

    evaluation_results[label] = {
        'Accuracy': accuracy,
        'Precision': report['1']['precision'],
        'Recall': report['1']['recall'],
        'F1-Score': report['1']['f1-score']
    }

    print(f"âœ… Evaluation for {label}:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {report['1']['precision']:.4f}")
    print(f"  Recall: {report['1']['recall']:.4f}")
    print(f"  F1-Score: {report['1']['f1-score']:.4f}")


print("\nâœ… Evaluation complete!")

# Optional: Display results in a DataFrame for better readability
results_balanced_df = pd.DataFrame(evaluation_results).T
display(results_balanced_df)

import joblib
import numpy as np # Import numpy to handle np.int64

# Load the TF-IDF vectorizer
tfidf_vectorizer = joblib.load("tfidf_vectorizer.pkl")

# Define a new comment
new_comment = "your are stupid person ."

# Preprocess the new comment (using the same functions defined earlier)
# Make sure clean_text and preprocess_text functions are available in this environment or redefine them if necessary.
cleaned_comment = clean_text(new_comment)
preprocessed_comment = preprocess_text(cleaned_comment)

# Vectorize the preprocessed comment
comment_vector = tfidf_vectorizer.transform([preprocessed_comment])

# Predict toxicity for each label using the balanced models
toxicity_predictions_balanced = {}
for label, model in balanced_lr_models.items():
    prediction = model.predict(comment_vector)
    # Convert np.int64 to standard Python int for cleaner output
    toxicity_predictions_balanced[label] = int(prediction[0])

# Display the predictions
print(f"Predictions for the comment: '{new_comment}' using balanced models")
print(toxicity_predictions_balanced)

import gensim
from gensim.models import Word2Vec
import pandas as pd

# Load the preprocessed data - Loading only a subset for Word2Vec training
df = pd.read_csv("preprocessed_train.csv", nrows=50000)

# Drop rows with NaN in 'clean_text' to ensure clean data for training
df.dropna(subset=['clean_text'], inplace=True)

# Prepare the data for Word2Vec training
# Word2Vec expects a list of lists, where each inner list is a list of words (a sentence)
# We will use the 'clean_text' column and split each comment into words
corpus = [comment.split() for comment in df['clean_text']]

# Train the Word2Vec model
# vector_size: Dimensionality of the word vectors.
# window: The maximum distance between the current and predicted word within a sentence.
# min_count: Ignores all words with total frequency lower than this.
# workers: Use these many worker threads to train the model (=faster training with multicore machines).
word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=5, workers=4)

# Build the vocabulary
word2vec_model.build_vocab(corpus)

# Train the model
word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=10)

print("âœ… Word2Vec model trained successfully!")

# You can test the model with word similarity
# print(word2vec_model.wv.most_similar('stupid'))

# Save the model for later use
word2vec_model.save("word2vec_model.model") # Corrected typo in filename
print("âœ… Word2Vec model saved as 'word2vec_model.model'")

import numpy as np
import pandas as pd
from gensim.models import Word2Vec

# Load the preprocessed data
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text' to ensure clean data
df.dropna(subset=['clean_text'], inplace=True)

# Load the trained Word2Vec model
word2vec_model = Word2Vec.load("word2vec_model.model")

# Function to average word vectors for a comment
def comment_to_vector(comment, model):
    words = comment.split()
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    if not word_vectors:
        return np.zeros(model.vector_size)  # Return zero vector if no words in vocabulary
    return np.mean(word_vectors, axis=0)

# Apply the function to create comment vectors
df['comment_vector'] = df['clean_text'].apply(lambda x: comment_to_vector(x, word2vec_model))

# Convert the list of vectors into a NumPy array
X_word2vec = np.vstack(df['comment_vector'].values)

print("âœ… Comment vectors created successfully!")
print("Shape of comment vectors:", X_word2vec.shape)

# You can display the first few comment vectors
# display(X_word2vec[:5])

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np # Import numpy to handle X_word2vec

# Assuming X_word2vec and df are available from previous steps

# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(
    X_word2vec, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully!")
print("Shape of X_train_w2v:", X_train_w2v.shape)
print("Shape of X_test_w2v:", X_test_w2v.shape)
print("Shape of y_train_w2v:", y_train_w2v.shape)
print("Shape of y_test_w2v:", y_test_w2v.shape)

!pip install tensorflow

from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd # Import pandas for chunking
from gensim.models import Word2Vec # Import Word2Vec to load the model
import joblib # Import joblib to save and load large numpy arrays

# Load the trained Word2Vec model
word2vec_model = Word2Vec.load("word2vec_model.model")

# Define maximum sequence length
max_sequence_length = 1250 # You can adjust this based on your data's typical comment length

# Function to get sequence of word vectors for a comment
def comment_to_sequence_vectors(comment, model, max_len):
    words = comment.split()
    word_vectors = [model.wv[word] for word in words if word in model.wv]

    # Pad or truncate the sequence
    if len(word_vectors) > max_len:
        word_vectors = word_vectors[:max_len]
    elif len(word_vectors) < max_len:
        # Pad with zero vectors
        padding = [np.zeros(model.vector_size)] * (max_len - len(word_vectors))
        word_vectors.extend(padding)

    return np.array(word_vectors)

# Apply the function to create sequence vectors in chunks
# This will take significantly longer than averaging
print("Creating sequence vectors in chunks (this might take some time)...")

chunk_size = 2000 # Define chunk size - adjusted to reduce memory usage
all_sequence_vectors = []

# Load the preprocessed data - Loading the full dataset now
df = pd.read_csv("preprocessed_train.csv") # Load the full dataframe here

# Drop rows with NaN in 'clean_text' to ensure clean data
df.dropna(subset=['clean_text'], inplace=True)


for i in range(0, len(df), chunk_size):
    print(f"Processing chunk {i//chunk_size + 1}/{len(df)//chunk_size + 1}...")
    chunk_df = df.iloc[i:i+chunk_size]
    chunk_sequence_vectors = chunk_df['clean_text'].apply(lambda x: comment_to_sequence_vectors(x, word2vec_model, max_sequence_length))
    all_sequence_vectors.extend(chunk_sequence_vectors.tolist()) # Extend with list of arrays

# Convert the list of sequence vectors into a NumPy array
# This will result in a 3D array: (num_samples, max_sequence_length, vector_size)
X_word2vec_sequences = np.array(all_sequence_vectors)

print("âœ… Comment sequence vectors created successfully!")
print("Shape of comment sequence vectors:", X_word2vec_sequences.shape)

# Save X_word2vec_sequences to a file
joblib.dump(X_word2vec_sequences, "X_word2vec_sequences.pkl")
print("âœ… X_word2vec_sequences saved to 'X_word2vec_sequences.pkl'")

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import joblib # Import joblib to load large numpy arrays

# Load the preprocessed dataset to get the labels and ensure df is defined
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text' to match X_word2vec_sequences
df.dropna(subset=['clean_text'], inplace=True)

# Load the created sequence vectors
try:
    X_word2vec_sequences = joblib.load("X_word2vec_sequences.pkl")
    print("âœ… X_word2vec_sequences loaded successfully!")
except FileNotFoundError:
    print("âŒ Error: X_word2vec_sequences.pkl not found. Please ensure the cell to create and save sequence vectors is executed first.")
    # You might want to add a mechanism here to stop execution or handle the error appropriately


# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
# We will perform the split here, but load data in batches during training
X_train_w2v_seq, X_test_w2v_seq, y_train_w2v_seq, y_test_w2v_seq = train_test_split(
    X_word2vec_sequences, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully with sequence vectors!")
print("Shape of X_train_w2v_seq:", X_train_w2v_seq.shape)
print("Shape of X_test_w2v_seq:", X_test_w2v_seq.shape)
print("Shape of y_train_w2v_seq:", y_train_w2v_seq.shape)
print("Shape of y_test_w2v_seq:", y_test_w2v_seq.shape)

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Assuming X_word2vec_sequences and df are available from previous steps

# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
# We will perform the split here, but load data in batches during training
X_train_w2v_seq, X_test_w2v_seq, y_train_w2v_seq, y_test_w2v_seq = train_test_split(
    X_word2vec_sequences, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully with sequence vectors!")
print("Shape of X_train_w2v_seq:", X_train_w2v_seq.shape)
print("Shape of X_test_w2v_seq:", X_test_w2v_seq.shape)
print("Shape of y_train_w2v_seq:", y_train_w2v_seq.shape)
print("Shape of y_test_w2v_seq:", y_test_w2v_seq.shape)

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Load the preprocessed dataset to get the labels and ensure df is defined
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text' to match X_word2vec_sequences
df.dropna(subset=['clean_text'], inplace=True)

# Assuming X_word2vec_sequences is available from the previous step

# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
# We will perform the split here, but load data in batches during training
X_train_w2v_seq, X_test_w2v_seq, y_train_w2v_seq, y_test_w2v_seq = train_test_split(
    X_word2vec_sequences, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully with sequence vectors!")
print("Shape of X_train_w2v_seq:", X_train_w2v_seq.shape)
print("Shape of X_test_w2v_seq:", X_test_w2v_seq.shape)
print("Shape of y_train_w2v_seq:", y_train_w2v_seq.shape)
print("Shape of y_test_w2v_seq:", y_test_w2v_seq.shape)

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# NOTE: Please ensure cell 71ead163 (Create Comment Sequence Vectors) is executed before this cell.

# Load the preprocessed dataset to get the labels and ensure df is defined
df = pd.read_csv("preprocessed_train.csv")

# Drop rows with NaN in 'clean_text' to match X_word2vec_sequences
df.dropna(subset=['clean_text'], inplace=True)

# Assuming X_word2vec_sequences is available from the previous step

# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
# We will perform the split here, but load data in batches during training
X_train_w2v_seq, X_test_w2v_seq, y_train_w2v_seq, y_test_w2v_seq = train_test_split(
    X_word2vec_sequences, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully with sequence vectors!")
print("Shape of X_train_w2v_seq:", X_train_w2v_seq.shape)
print("Shape of X_test_w2v_seq:", X_test_w2v_seq.shape)
print("Shape of y_train_w2v_seq:", y_train_w2v_seq.shape)
print("Shape of y_test_w2v_seq:", y_test_w2v_seq.shape)

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Assuming X_word2vec_sequences and df are available from previous steps

# Define labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y = df[labels]

# Split data
# We will perform the split here, but load data in batches during training
X_train_w2v_seq, X_test_w2v_seq, y_train_w2v_seq, y_test_w2v_seq = train_test_split(
    X_word2vec_sequences, y, test_size=0.2, random_state=42
)

print("âœ… Data split successfully with sequence vectors!")
print("Shape of X_train_w2v_seq:", X_train_w2v_seq.shape)
print("Shape of X_test_w2v_seq:", X_test_w2v_seq.shape)
print("Shape of y_train_w2v_seq:", y_train_w2v_seq.shape)
print("Shape of y_test_w2v_seq:", y_test_w2v_seq.shape)

"""### Build the LSTM Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import numpy as np

# Define the model parameters
lstm_units = 64
dense_units = 32
dropout_rate = 0.3
learning_rate = 0.001
num_classes = len(labels) # Should be 6

# Assuming X_train_w2v_seq has shape (num_samples, max_sequence_length, vector_size)
# The input_shape for the LSTM should be (max_sequence_length, vector_size)
# Get max_sequence_length and vector_size from the shape of X_train_w2v_seq
max_sequence_length = X_train_w2v_seq.shape[1]
vector_size = X_train_w2v_seq.shape[2]


model_lstm = Sequential()
# Add an Input layer to explicitly define the input shape
model_lstm.add(Input(shape=(max_sequence_length, vector_size)))
model_lstm.add(LSTM(lstm_units, return_sequences=False)) # return_sequences=False for a single output per sequence
model_lstm.add(Dropout(dropout_rate))
model_lstm.add(Dense(dense_units, activation='relu'))
model_lstm.add(Dropout(dropout_rate))
model_lstm.add(Dense(num_classes, activation='sigmoid')) # Sigmoid for multi-label classification

# Compile the model
optimizer = Adam(learning_rate=learning_rate)
model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

model_lstm.summary()

"""### Train the LSTM Model"""

from tensorflow.keras.callbacks import Callback
import numpy as np

# Custom Data Generator to load data in batches during training
class DataGenerator(Callback):
    def __init__(self, X_data, y_data, batch_size):
        self.X_data = X_data
        self.y_data = y_data
        self.batch_size = batch_size
        self.indices = np.arange(len(self.X_data))
        self.on_epoch_end()

    def __len__(self__(self):
        return int(np.floor(len(self.X_data) / self.batch_size))

    def __getitem__(self, index):
        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        X_batch = self.X_data[indices]
        y_batch = self.y_data.iloc[indices] # Use .iloc for pandas DataFrame
        return X_batch, y_batch

    def on_epoch_end(self):
        np.random.shuffle(self.indices)


# Define training parameters
epochs = 5 # You can adjust this
batch_size = 64 # You can adjust this

# Create data generators for training and validation
train_generator = DataGenerator(X_train_w2v_seq, y_train_w2v_seq, batch_size)
# For validation, we can also use a generator or load the full validation set if it fits in memory
# Let's create a generator for consistency
# Split the training data further for validation set for the generator
# Or, create a generator for X_test_w2v_seq and y_test_w2v_seq if you want to use the test set as validation
# For now, let's stick to using a validation split during model.fit, which Keras handles internally for arrays

# Train the model using the data generator
print("ðŸš€ Training the LSTM model with data generator...")
history = model_lstm.fit(
    train_generator,
    epochs=epochs,
    validation_data=(X_test_w2v_seq, y_test_w2v_seq), # Use the test set as validation data
    verbose=1
)

print("\nâœ… LSTM model training complete!")

"""### Evaluate the LSTM Model"""

from sklearn.metrics import classification_report, accuracy_score

# Evaluate the model on the test set
print("ðŸš€ Evaluating the LSTM model...")
# Use the test data directly for evaluation
loss, accuracy = model_lstm.evaluate(X_test_w2v_seq, y_test_w2v_seq, verbose=0)

print(f"\nâœ… Test Loss: {loss:.4f}")
print(f"âœ… Test Accuracy: {accuracy:.4f}")

# Get predictions for classification report
y_pred_probs = model_lstm.predict(X_test_w2v_seq)
# Convert probabilities to binary predictions (0 or 1)
y_pred = (y_pred_probs > 0.5).astype(int)

# Generate classification report for each label
print("\nâœ… Classification Report for LSTM Model:")
for i, label in enumerate(labels):
    print(f"\n--- {label} ---")
    # Ensure y_test_w2v_seq is a numpy array or select the column correctly
    print(classification_report(y_test_w2v_seq.iloc[:, i], y_pred[:, i]))
